<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Navigation Meets Language Models: Enhancing Zero-shot Object
Navigation with Vision-Language Models and Tree of Thoughts</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <div class="container">
        <h1 style="text-align: center; font-family: Arial;">VLTNet:</h1>
        <h2 style="text-align: center; font-family: Arial;">Enhancing Zero-shot Object Navigation with Vision-Language Models and Tree of Thoughts</h2>
        <h3 style="text-align: center; font-family: Arial; font-size: small;">CongCong Wen<sup>1*</sup>, Yisiyuan Huang<sup>2*</sup>, Yanjia Huang<sup>2</sup>, Wenyu Han<sup>2</sup>,<br>Shuaihang Yuan<sup>1</sup>, Yu-Shen Liu<sup>3</sup> and Yi Fang<sup>1</sup></h3>
        <!-- Introduction Section -->
        <img src="fig_intro5.png" alt="intro-image" class="intro-image" width="500">
        <section class="Abstract">
            <!-- <h2>Abstract</h2> -->
            <p> Object navigation is crucial for robots, but tra-
ditional methods require substantial training data and cannot
generalize to unknown environments. Zero-shot object naviga-
tion (ZSON) aims to address this challenge, enabling robots
to interact with unfamiliar objects without specific training
data. Language-driven zero-shot object navigation (L-ZSON)
is an extension of ZSON that incorporates natural language
instructions to guide robot navigation and interaction with
objects. In this paper, we propose VLTNet for L-ZSON, a
novel Vision Language model with a Tree-of-thought NETwork.
VLTNet comprises four main modules: vision language model
understanding, semantic mapping, tree-of-thought reasoning
and exploration, and goal identification. Among these mod-
ules, tree-of-thought (ToT) reasoning and exploration serve as
core components, innovatively using the ToT reasoning frame-
work for frontier selection in robot exploration. Compared
to traditional Large Language Models (LLMs), ToT reason-
ing involves multi-path reasoning processes and backtracking
when necessary, enabling globally informed decision-making.
Experimental results on benchmarks such as PASTURE and
RoboTHOR demonstrate the outstanding performance of our
model in LZSON, particularly in scenarios involving complex
                natural language as goal instructions.</p>
            
        </section>

        <!-- Content Section -->
        <section class="content">
            <h1>Approach</h1>
            <img src="fig_overview6.png" alt="pipeline-image" width="1000" class="pipeline-image" >
            <p>VLTNet coordinates four main modules to tackle the L-ZSON tasks</p>
            <h2>Vision Language Model Understanding Module</h2>
            <p>Lorem ipsum, dolor sit amet consectetur adipisicing elit. Magnam sapiente laboriosam, nihil tempora dolores libero at, dignissimos, explicabo impedit nemo incidunt aut atque! Nulla porro, illo quidem aliquid ullam itaque!</p>
            <h2>Semantic Mapping Module</h2>
            <p>Lorem ipsum, dolor sit amet consectetur adipisicing elit. Magnam sapiente laboriosam, nihil tempora dolores libero at, dignissimos, explicabo impedit nemo incidunt aut atque! Nulla porro, illo quidem aliquid ullam itaque!</p>

            <h2>Tree of Thoughts Exploration Module</h2>
            <p>Lorem ipsum, dolor sit amet consectetur adipisicing elit. Magnam sapiente laboriosam, nihil tempora dolores libero at, dignissimos, explicabo impedit nemo incidunt aut atque! Nulla porro, illo quidem aliquid ullam itaque!</p>

            <h2>Goal identification Module</h2>
            <p>Lorem ipsum, dolor sit amet consectetur adipisicing elit. Magnam sapiente laboriosam, nihil tempora dolores libero at, dignissimos, explicabo impedit nemo incidunt aut atque! Nulla porro, illo quidem aliquid ullam itaque!</p>

            

            <div class="video-section">
                <h2>Trajectory visualization:</h2>
                <p>Here we give a visualized comparison between VLTNet and ESC on different benchmarks in PASTURE Dataset.</p>
                <h1>VLTNet</h1>
                <video controls class="demo-video">
                    <source src="longtail.mp4" type="video/mp4">
                </video>

                <video controls class="demo-video">
                    <source src="spatial.mp4" type="video/mp4">
                </video>

                <video controls class="demo-video">
                    <source src="appearance.mp4" type="video/mp4">
                </video>
                <h1>ESC</h1>
                <video controls class="demo-video">
                    <source src=".mp4" type="video/mp4">
                </video>

                <video controls class="demo-video">
                    <source src=".mp4" type="video/mp4">
                </video>

                <video controls class="demo-video">
                    <source src=".mp4" type="video/mp4">
                </video>
            </div>
            
        </section>
    </div>
</body>

</html>

